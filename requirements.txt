# Core Dependencies - Install torch first
torch>=2.1.0

# Flash Attention - Install after torch with special flags
# Run: pip install flash-attn --no-build-isolation
flash-attn>=2.3.0

# Then install transformers ecosystem
transformers>=4.36.0
accelerate>=0.25.0
peft>=0.7.0
trl>=0.7.0
deepspeed>=0.12.0

# Dataset Preparation
pyyaml>=6.0
tqdm>=4.65.0
datasets>=2.14.0

# Monitoring
wandb>=0.16.0
